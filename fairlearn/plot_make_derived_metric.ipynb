{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fairlearn\n",
      "  Downloading fairlearn-0.7.0-py3-none-any.whl (177 kB)\n",
      "\u001b[K     |████████████████████████████████| 177 kB 6.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.1 in /opt/conda/lib/python3.6/site-packages (from fairlearn) (0.22.2)\n",
      "Requirement already satisfied: pandas>=0.25.1 in /opt/conda/lib/python3.6/site-packages (from fairlearn) (0.25.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.6/site-packages (from fairlearn) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.6/site-packages (from fairlearn) (1.18.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn>=0.22.1->fairlearn) (0.14.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from pandas>=0.25.1->fairlearn) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas>=0.25.1->fairlearn) (2018.9)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas>=0.25.1->fairlearn) (1.14.0)\n",
      "Installing collected packages: fairlearn\n",
      "Successfully installed fairlearn-0.7.0\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fairlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making Derived Metrics\n",
    "======================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the use of the\n",
    "fairlearn.metrics.make\\_derived\\_metric function. Many higher-order\n",
    "machine learning algorithms (such as hyperparameter tuners) make use of\n",
    "scalar metrics when deciding how to proceed. While the\n",
    "fairlearn.metrics.MetricFrame has the ability to produce such scalars\n",
    "through its aggregation functions, its API does not conform to that\n",
    "usually expected by these algorithms. The\n",
    "\\~fairlearn.metrics.make\\_derived\\_metric function exists to bridge this\n",
    "gap.\n",
    "\n",
    "Getting the Data\n",
    "================\n",
    "\n",
    "*This section may be skipped. It simply creates a dataset for\n",
    "illustrative purposes*\n",
    "\n",
    "We will use the well-known UCI 'Adult' dataset as the basis of this\n",
    "demonstration. This is not for a lending scenario, but we will regard it\n",
    "as one for the purposes of this example. We will use the existing 'race'\n",
    "and 'sex' columns (trimming the former to three unique values), and\n",
    "manufacture credit score bands and loan sizes from other columns. We\n",
    "start with some uncontroversial import statements:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import numpy as np\n",
    "\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.pipeline import Pipeline\n",
    "from fairlearn.metrics import MetricFrame, make_derived_metric\n",
    "from fairlearn.metrics import accuracy_score_group_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we import the data, dropping any rows which are missing data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data = fetch_openml(data_id=1590, as_frame=True)\n",
    "X_raw = data.data\n",
    "y = (data.target == \">50K\") * 1\n",
    "A = X_raw[[\"race\", \"sex\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to preprocess the data. Before applying any transforms,\n",
    "we first split the data into train and test sets. All the transforms we\n",
    "apply will be trained on the training set, and then applied to the test\n",
    "set. This ensures that data doesn't leak between the two sets (this is a\n",
    "serious but subtle [problem in machine\n",
    "learning](https://en.wikipedia.org/wiki/Leakage_(machine_learning))).\n",
    "So, first we split the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test, A_train, A_test) = train_test_split(\n",
    "    X_raw, y, A, test_size=0.3, random_state=12345, stratify=y\n",
    ")\n",
    "\n",
    "# Ensure indices are aligned between X, y and A,\n",
    "# after all the slicing and splitting of DataFrames\n",
    "# and Series\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "A_train = A_train.reset_index(drop=True)\n",
    "A_test = A_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we build two \\~sklearn.pipeline.Pipeline objects to process the\n",
    "columns, one for numeric data, and the other for categorical data. Both\n",
    "impute missing values; the difference is whether the data are scaled\n",
    "(numeric columns) or one-hot encoded (categorical columns). Imputation\n",
    "of missing values should generally be done with care, since it could\n",
    "potentially introduce biases. Of course, removing rows with missing data\n",
    "could also cause trouble, if particular subgroups have poorer data\n",
    "quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"impute\", SimpleImputer()),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "categorical_transformer = Pipeline(\n",
    "    [\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, selector(dtype_exclude=\"category\")),\n",
    "        (\"cat\", categorical_transformer, selector(dtype_include=\"category\")),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our preprocessor defined, we can now build a new pipeline which\n",
    "includes an Estimator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "unmitigated_predictor = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\n",
    "            \"classifier\",\n",
    "            LogisticRegression(solver=\"liblinear\", fit_intercept=True),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the pipeline fully defined, we can first train it with the training\n",
    "data, and then generate predictions from the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "unmitigated_predictor.fit(X_train, y_train)\n",
    "y_pred = unmitigated_predictor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a derived metric\n",
    "=========================\n",
    "\n",
    "Suppose our key metric is the accuracy score, and we are most interested\n",
    "in ensuring that it exceeds some threshold for all subgroups We might\n",
    "use the \\~fairlearn.metrics.MetricFrame as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum accuracy_score:  0.8098103202121151\n"
     ]
    }
   ],
   "source": [
    "acc_frame = MetricFrame(\n",
    "    metrics=skm.accuracy_score,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    sensitive_features=A_test[\"sex\"]\n",
    ")\n",
    "print(\"Minimum accuracy_score: \", acc_frame.group_min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a function to perform this in a single call using\n",
    "\\~fairlearn.metrics.make\\_derived\\_metric. This takes the following\n",
    "arguments (which must always be supplied as keyword arguments):\n",
    "\n",
    "-   `metric=`{.sourceCode}, the base metric function\n",
    "-   `transform=`{.sourceCode}, the name of the aggregation\n",
    "    transformation to perform. For this demonstration, we want this to\n",
    "    be `'group_min'`{.sourceCode}\n",
    "-   `sample_param_names=`{.sourceCode}, a list of parameter names which\n",
    "    should be treated as sample parameters. This is optional, and\n",
    "    defaults to `['sample_weight']`{.sourceCode} which is appropriate\n",
    "    for many metrics in scikit-learn.\n",
    "\n",
    "The result is a new function with the same signature as the base metric,\n",
    "which accepts two extra arguments:\n",
    "\n",
    "> -   `sensitive_features=`{.sourceCode} to specify the sensitive\n",
    ">     features which define the subgroups\n",
    "> -   `method=`{.sourceCode} to adjust how the aggregation\n",
    ">     transformation operates. This corresponds to the same argument in\n",
    ">     fairlearn.metrics.MetricFrame.difference and\n",
    ">     fairlearn.metrics.MetricFrame.ratio\n",
    "\n",
    "For the current case, we do not need the `method=`{.sourceCode}\n",
    "argument, since we are taking the minimum value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum accuracy_score:  0.8098103202121151\n"
     ]
    }
   ],
   "source": [
    "my_acc = make_derived_metric(metric=skm.accuracy_score, transform=\"group_min\")\n",
    "my_acc_min = my_acc(y_test, y_pred, sensitive_features=A_test[\"sex\"])\n",
    "print(\"Minimum accuracy_score: \", my_acc_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show that the returned function also works with sample weights:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From MetricFrame: 0.8132795888539268\n",
      "From function   : 0.8132795888539268\n"
     ]
    }
   ],
   "source": [
    "random_weights = np.random.rand(len(y_test))\n",
    "\n",
    "acc_frame_sw = MetricFrame(\n",
    "    metrics=skm.accuracy_score,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    sensitive_features=A_test[\"sex\"],\n",
    "    sample_params={\"sample_weight\": random_weights},\n",
    ")\n",
    "\n",
    "from_frame = acc_frame_sw.group_min()\n",
    "from_func = my_acc(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    sensitive_features=A_test[\"sex\"],\n",
    "    sample_weight=random_weights,\n",
    ")\n",
    "\n",
    "print(\"From MetricFrame:\", from_frame)\n",
    "print(\"From function   :\", from_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned function can also handle parameters which are not sample\n",
    "parameters. Consider sklearn.metrics.fbeta\\_score, which has a required\n",
    "`beta=`{.sourceCode} argument (and suppose that this time we are most\n",
    "interested in the maximum difference to the overall value). First we\n",
    "evaluate this with a fairlearn.metrics.MetricFrame:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From frame: 0.03034662666981003\n"
     ]
    }
   ],
   "source": [
    "fbeta_03 = functools.partial(skm.fbeta_score, beta=0.3)\n",
    "fbeta_03.__name__ = \"fbeta_score__beta_0.3\"\n",
    "\n",
    "beta_frame = MetricFrame(\n",
    "    metrics=fbeta_03,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    sensitive_features=A_test[\"sex\"],\n",
    "    sample_params={\"sample_weight\": random_weights},\n",
    ")\n",
    "beta_from_frame = beta_frame.difference(method=\"to_overall\")\n",
    "\n",
    "print(\"From frame:\", beta_from_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And next, we create a function to evaluate the same. Note that we do not\n",
    "need to use functools.partial to bind the `beta=`{.sourceCode} argument:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From function: 0.03034662666981003\n"
     ]
    }
   ],
   "source": [
    "beta_func = make_derived_metric(metric=skm.fbeta_score, transform=\"difference\")\n",
    "\n",
    "beta_from_func = beta_func(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    sensitive_features=A_test[\"sex\"],\n",
    "    beta=0.3,\n",
    "    sample_weight=random_weights,\n",
    "    method=\"to_overall\",\n",
    ")\n",
    "\n",
    "print(\"From function:\", beta_from_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pregenerated Metrics\n",
    "====================\n",
    "\n",
    "We provide a number of pregenerated metrics, to cover common use cases.\n",
    "For example, we provide a `accuracy_score_group_min()`{.sourceCode}\n",
    "function to find the minimum over the accuracy scores:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From my function : 0.8114035087719298\n",
      "From pregenerated: 0.8114035087719298\n"
     ]
    }
   ],
   "source": [
    "from_myacc = my_acc(y_test, y_pred, sensitive_features=A_test[\"race\"])\n",
    "\n",
    "from_pregen = accuracy_score_group_min(\n",
    "    y_test, y_pred, sensitive_features=A_test[\"race\"]\n",
    ")\n",
    "\n",
    "print(\"From my function :\", from_myacc)\n",
    "print(\"From pregenerated:\", from_pregen)\n",
    "assert from_myacc == from_pregen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
